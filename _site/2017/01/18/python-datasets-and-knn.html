<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Introduction to Machine Learning with Python - Chapter 2 - Datasets and kNN</title>
  <meta name="description" content="```pythonimport sysprint(“Python version: {}”.format(sys.version))import pandas as pdprint(“pandas version: {}”.format(pd.version))import matplotlibprint(“ma...">

  
  
  <link rel="stylesheet" href="http://localhost:4000/assets/style.css">

  <link rel="canonical" href="http://localhost:4000/2017/01/18/python-datasets-and-knn.html">
  <link rel="alternate" type="application/rss+xml" title="Elvin Ouyang's Blog" href="http://localhost:4000/feed.xml">

  <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>

  
  <!-- font-smoothing is only applied on dark themes -->
  <body class="font-smoothing">

    <header class="px-2 clearfix">
  <!-- <div class="left-lg absolute-lg left-0 top-0 sm-width-full mt-2">
    <a class="no-underline-hover px-1" href="/">
      <span class="inline-block h4 hide-sm ml-2">&#x261c;</span>
    </a>
    <a class="italic no-underline" href="/">
       home
    </a>
  </div> -->
  <div class="right-lg absolute-lg right-0 top-0">
    <ul class="mt-1 mt-lg-2 mr-2 mr-lg-3">
      <li class="inline-block block-lg text-right ml-1 ml-lg-0">
        <a class="italic h4 bold no-underline" href="/">
          <!-- Elvin Ouyang's Blog -->
          Home
        </a>
      </li>
      
        
        <li class="inline-block block-lg text-right ml-1 ml-lg-0">
          <a class="italic no-underline h4" href="/about/">
            About
          </a>
        </li>
        
      
        
      
        
        <li class="inline-block block-lg text-right ml-1 ml-lg-0">
          <a class="italic no-underline h4" href="/projects/">
            Projects
          </a>
        </li>
        
      
        
        <li class="inline-block block-lg text-right ml-1 ml-lg-0">
          <a class="italic no-underline h4" href="/resources/">
            Resources
          </a>
        </li>
        
      
        
        <li class="inline-block block-lg text-right ml-1 ml-lg-0">
          <a class="italic no-underline h4" href="/study-notes/">
            Study Notes
          </a>
        </li>
        
      
        
      
    </ul>
  </div>
</header>


    <div>
      <article class="container mx-auto px-2" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="mt-4 mb-2 clearfix header-text">
    <h1 class="h0 inline-block py-2 mt-4 header-title">Introduction to Machine Learning with Python - Chapter 2 - Datasets and kNN</h1>
    <div class="clearfix mb-4 py-1">
      <p class="h4 lh-condensed"><time datetime="2017-01-18T00:00:00-05:00" itemprop="datePublished">Jan 18, 2017</time></p>
      <div class="col-1 sm-width-full border-top-thick">
      </div>
    </div>
  </div>

  <div class="prose py-4" itemprop="articleBody">
      <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Python version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">version</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="k">print</span><span class="p">(</span><span class="s">"pandas version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="k">print</span><span class="p">(</span><span class="s">"matplotlib version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="k">print</span><span class="p">(</span><span class="s">"NumPy version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="kn">as</span> <span class="nn">sp</span>
<span class="k">print</span><span class="p">(</span><span class="s">"SciPy version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">IPython</span>
<span class="k">print</span><span class="p">(</span><span class="s">"IPython version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">IPython</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="k">print</span><span class="p">(</span><span class="s">"scikit-learn version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sklearn</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">mglearn</span>
<span class="k">print</span><span class="p">(</span><span class="s">"mglearn version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mglearn</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Python version: 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
pandas version: 0.19.2
matplotlib version: 1.5.3
NumPy version: 1.11.3
SciPy version: 0.18.1
IPython version: 5.1.0
scikit-learn version: 0.18.1
mglearn version: 0.1.3
</code></pre>
</div>

<h1 id="1-supervised-learning">1 Supervised Learning</h1>

<blockquote>
  <p>Remember that supervised learning is used whenever we want to predict a certain
outcome from a given input, and we have examples of input/output pairs. We build a
machine learning model from these input/output pairs, which comprise our training
set. Our goal is to make accurate predictions for new, never-before-seen data. Supervised
learning often requires <strong>human effort to build the training set</strong>, but afterward
automates and often speeds up an otherwise laborious or infeasible task.</p>
</blockquote>

<h2 id="11-supervised-learning-categories">1.1 Supervised Learning Categories</h2>
<p>Two major forms of supervised learning:</p>
<ul>
  <li>Classification</li>
  <li>Regression</li>
</ul>

<p>For <strong>classification</strong>, there are two possible forms:</p>
<blockquote>
  <p>Classification is sometimes separated into <strong>binary classification</strong>,
which is the special case of distinguishing between exactly two classes, and <strong>multiclass
classification</strong>, which is classification between more than two classes. You can think of
binary classification as trying to answer a yes/no question.</p>
</blockquote>

<p>For <strong>regression</strong>, the model tries to predict a continuous target value:</p>
<blockquote>
  <p>For regression tasks, the goal is to predict a continuous number, or a floating-point
number in programming terms (or real number in mathematical terms).</p>
</blockquote>

<blockquote>
  <p>An easy way to distinguish between classification and regression tasks is to ask
whether there is some kind of continuity in the output. If there is continuity between
possible outcomes, then the problem is a regression problem.</p>
</blockquote>

<h2 id="12-overfitting-vs-underfitting">1.2 Overfitting vs. Underfitting</h2>

<blockquote>
  <p>Building a model that is too complex for the amount of
information we have, as our novice data scientist did, is called <strong>overfitting</strong>. Overfitting
occurs when you fit a model too closely to the particularities of the training set and
obtain a model that works well on the training set but is not able to generalize to new
data.</p>
</blockquote>

<blockquote>
  <p>Choosing
too simple a model is called <strong>underfitting</strong>.</p>
</blockquote>

<p>In order to strike a balance between overfitting and underfitting, we need to find a <strong>sweet spot</strong> in the model complexity vs. accuray for training and generalization.</p>

<p>On the other hand, we should be aware that larget dataset size itself will lead to bigger model complexity, turning the sweet spot further right on the complexity spectrum.</p>

<blockquote>
  <p>In the real world, you often have the ability to decide how
much data to collect, which might be more beneficial than tweaking and tuning your
model. Never underestimate the power of more data.</p>
</blockquote>

<h3 id="sample-datasets">Sample Datasets</h3>

<p><strong>Dataset One:</strong> cancer database</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"cancer.keys(): </span><span class="se">\n</span><span class="s">{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>cancer.keys():
dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Shape of cancer data: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Shape of cancer data: (569, 30)
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Sample counts per class:</span><span class="se">\n</span><span class="s">{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="p">{</span><span class="n">n</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span>
                             <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">))}))</span>
<span class="c"># format() includes n:v in the form of zip(n, v)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Sample counts per class:
{'malignant': 212, 'benign': 357}
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Feature names:</span><span class="se">\n</span><span class="s">{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Feature names:
['mean radius' 'mean texture' 'mean perimeter' 'mean area'
 'mean smoothness' 'mean compactness' 'mean concavity'
 'mean concave points' 'mean symmetry' 'mean fractal dimension'
 'radius error' 'texture error' 'perimeter error' 'area error'
 'smoothness error' 'compactness error' 'concavity error'
 'concave points error' 'symmetry error' 'fractal dimension error'
 'worst radius' 'worst texture' 'worst perimeter' 'worst area'
 'worst smoothness' 'worst compactness' 'worst concavity'
 'worst concave points' 'worst symmetry' 'worst fractal dimension']
</code></pre>
</div>

<p><strong>Dataset Two:</strong> Boston Housing</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Data shape: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Data shape: (506, 13)
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_extended_boston</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"X.shape: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>X.shape: (506, 104)
</code></pre>
</div>

<h2 id="13-k-nearest-neighbors">1.3 k-Nearest Neighbors</h2>

<blockquote>
  <p>The k-NN algorithm is arguably the simplest machine learning algorithm. Building
the model consists only of storing the training dataset. To make a prediction for a
new data point, the algorithm finds the closest data points in the training dataset—its
“nearest neighbors.”</p>
</blockquote>

<p>The <strong>simplest</strong> version is to choose one closest point of a new unlabeled data to give the predicted target. This is 1-NN.</p>

<p>For more completicated modelling, we can choose <em>k</em> nearest neighbors (i.e. k-NN) and conduct <em>voting</em> to determine the result. The <em>voting</em> involves counting the frequency of different label options for the k points and give the most frequent label to the new data point.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># 1-NN</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_knn_classification</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

</code></pre>
</div>

<p><img src="/assets/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_files/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_18_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># 3-NN</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_knn_classification</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre>
</div>

<p><img src="/assets/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_files/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_19_0.png" alt="png" /></p>

<h3 id="131-steps-to-apply-a-k-nn-model">1.3.1 Steps to apply a k-NN model</h3>

<p><strong>First</strong> we create a training set and a testing set.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_forge</span><span class="p">()</span>
<span class="c"># Use the textbook package to create a sample data set</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c"># Create the training and testing sets</span>
</code></pre>
</div>

<p><strong>Second</strong> we instantiate a k-NN class and fit with our training set.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c"># Instantiate a k-NN model with 3 nearest neighbors</span>

<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c"># Fit the model, clf, to the training set. For k-NN, it's simply</span>
<span class="c"># storing the data</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=3, p=2,
           weights='uniform')
</code></pre>
</div>

<p><strong>Third</strong> we make a prediction based on the test set (i.e. X_test)</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Test set predictions: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
<span class="c"># We calculate the predictions for y_test with the clf model</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Test set predictions: [1 0 1 0 1 0 0]
</code></pre>
</div>

<p><strong>Fourth</strong> we evaluate the accuracy of the model by comparing the predictions with <em>“correct answers”</em>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Test set accuracy: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Test set accuracy: 0.86
</code></pre>
</div>

<p>The model has an accuracy of 86%.</p>

<h3 id="132-analyzing-kneighborsclassifier">1.3.2 Analyzing KNeighborsClassifier</h3>

<p>To further examine the effectiveness of k-NN models, we should check out the <strong>decision boundary</strong> of k-NN models on the same dataset but with different k values. Below we will check the visualizations of 1-NN to 9-NN.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">n_neighbors</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="c"># the fit method returns the object self, so we can instantiate</span>
    <span class="c"># and fit in one line</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbors</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"{} neighbor(s)"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"feature 0"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"feature 1"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x7fe1c7947240&gt;
</code></pre>
</div>

<p><img src="/assets/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_files/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_31_1.png" alt="png" /></p>

<p>In the case of <em>k-NN</em> models, the more neighbors used, the simpler the model is (represented by the smoother boundry for higher k value above).</p>

<h3 id="133-knn-accuracy-on-breast-cancer-data">1.3.3 kNN accuracy on Breast Cancer data</h3>

<p>We now test the kNN model on the real world <strong>breast cancer dataset</strong>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>

<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">66</span><span class="p">)</span>
<span class="c"># Create training and testing datasets</span>

<span class="n">training_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c"># try n_neighbors from 1 to 10</span>
<span class="n">neighbors_settings</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n_neighbors</span> <span class="ow">in</span> <span class="n">neighbors_settings</span><span class="p">:</span>
    <span class="c"># build the model</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbors</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c"># record training accuracy</span>
    <span class="n">training_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="c"># record generalization accuracy</span>
    <span class="n">test_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors_settings</span><span class="p">,</span>
         <span class="n">training_accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"training accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors_settings</span><span class="p">,</span>
        <span class="n">test_accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"test accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"n_neighbors"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x7fe1c78ae780&gt;
</code></pre>
</div>

<p><img src="/assets/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_files/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_35_1.png" alt="png" /></p>

<p>From above graph we can observe that the accuracy on the test set is best around k=6. Another thing to be noted is that since kNN models is the most complex when k=1, the trends of the two lines are flipped compared to standard complexity-accuracy chart for models.</p>

<h3 id="134-k-neighbors-regression-variant-model">1.3.4 k-neighbors regression variant model</h3>

<p>A k-neighbors regression model fetches the target value (<em>continuous target variable</em>) of the <em>k</em> nearest neighbors and calculate the mean of those target values as the predicted target value.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_knn_regression</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<p><img src="/assets/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_files/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_39_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_knn_regression</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre>
</div>

<p><img src="/assets/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_files/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_40_0.png" alt="png" /></p>

<p>To use k-neighbors regression model, one should instantiate a <strong>KNeighborsRegressor</strong> class in scikit-learn.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_wave</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>

<span class="c"># split the wave dataset into a training and test set</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c"># Instantiate the model and set the number of neighbors to 3</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c"># fit the model using the training data and training targets</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
          metric_params=None, n_jobs=1, n_neighbors=3, p=2,
          weights='uniform')
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Test set predictions: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Test set predictions: [-0.05396539  0.35686046  1.13671923 -1.89415682 -1.13881398 -1.63113382
  0.35686046  0.91241374 -0.44680446 -1.13881398]
</code></pre>
</div>

<p>The <em>.score()</em> method for KNeighborsRegressor returns the R-square of the model, with 1 representing a perfect prediction and 0 representing a constant target value regardless of the feature values.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Test set R^2: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Test set R^2: 0.83
</code></pre>
</div>

<p>To visualize the changes in the model when <em>k</em> increases, we compare k-Neighbors Regressor model when k equals 1, 3, and 9.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c"># Create 1,000 data points, evenly spaced between -3 and 3</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n_neighbors</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="c"># make predictions using 1, 3, or 9 neighbors</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbors</span><span class="p">)</span>
    <span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">line</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s">"^"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s">'v'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
        <span class="s">"{} neighbor(s)</span><span class="se">\n</span><span class="s"> train score: {:.2f} test score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">n_neighbors</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span>
            <span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Feature"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Target"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s">"Model predictions"</span><span class="p">,</span> <span class="s">"Training data/target"</span><span class="p">,</span>
            <span class="s">"Test data/target"</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">"best"</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x7fe1c7090470&gt;
</code></pre>
</div>

<p><img src="/assets/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_files/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_47_1.png" alt="png" /></p>

<p>The predicted line becomes significantly smoother when the count of neighbors increases. It signifies a reduction in complexity compared to 1-NR and 3-NR.</p>

<p>We can then move forward to use similar methods to evaluate the <strong>“sweet spot”</strong> for <em>k</em> value in this model.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_wave</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">608</span><span class="p">)</span>
<span class="c"># Create training and testing datasets</span>

<span class="n">training_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c"># try n_neighbors from 1 to 10</span>
<span class="n">neighbors_settings</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n_neighbors</span> <span class="ow">in</span> <span class="n">neighbors_settings</span><span class="p">:</span>
    <span class="c"># build the model</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbors</span><span class="p">)</span>
    <span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c"># record training accuracy</span>
    <span class="n">training_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="c"># record generalization accuracy</span>
    <span class="n">test_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors_settings</span><span class="p">,</span>
         <span class="n">training_accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"training accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors_settings</span><span class="p">,</span>
        <span class="n">test_accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"test accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"n_neighbors"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x7fe1c7090f98&gt;
</code></pre>
</div>

<p><img src="/assets/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_files/Introduction%20to%20Machine%20Learning%20with%20Python%20-%20Chapter%202%20-%20Datasets%20and%20kNN_49_1.png" alt="png" /></p>

<p>We can observe from above graph that the increase of <em>k</em> result in a reduction in training accuracy. It is also interesting to see that the accuracy for test set increases significantly, to a point it even surpasses training set accuracy. It certainly seems that at the point where <em>k</em> equals around 10 we can see the best model setting for k-NR.</p>

<h2 id="14-k-nn-pros-and-cons">1.4 k-NN pros and cons</h2>

<p><strong>Parameters of k-NN</strong></p>
<blockquote>
  <p>In principle, there are <strong>two important parameters</strong> to the KNeighbors classifier: the
<strong>number of neighbors</strong> and <strong>how you measure distance between data points</strong>. In practice,
using a small number of neighbors like three or five often works well, but you should
certainly adjust this parameter.</p>
</blockquote>

<p><strong>Pros</strong></p>
<blockquote>
  <p>One of the strengths of k-NN is that the <strong>model is very easy to understand</strong>, and often
<strong>gives reasonable performance</strong> without a lot of adjustments. Using this algorithm is a
<strong>good baseline method</strong> to try before considering more advanced techniques.</p>
</blockquote>

<p><strong>Cons</strong></p>
<blockquote>
  <p>Building
the nearest neighbors model is usually very fast, but when your <strong>training set is very
large</strong> (either in number of features or in number of samples) <strong>prediction can be slow</strong>.</p>
</blockquote>

<blockquote>
  <p>This approach often does not perform well on <strong>datasets with many features</strong>
(hundreds or more), and it does particularly badly with datasets where most features
are 0 most of the time (so-called <strong>sparse datasets</strong>). Therefore this model is not good for practices such as text mining.</p>
</blockquote>

  </div>
</article>

<div class="container mx-auto px-2 py-2 clearfix">
  <!-- Use if you want to show previous and next for all posts. -->



  <div class="col-4 sm-width-full left mr-lg-4 mt-3">
    <a class="no-underline-hover py-1 block" href="http://localhost:4000/2017/01/15/introduction-to-machine-learning-with-python-chapter-1.html">
      <span class="h5 bold">Previous</span>
      <p class="bold h3 link-primary mb-1">Introduction to Machine Learning with Python - Chapter 1</p>
      <p>Essential Tools for Python Machine Learning NumPy Arrays %matplotlib inline import numpy as np x = np.array([[1,2,3],[4,5,6]]) print("x:\n{}".format(x)) # .format()...</p>
    </a>
  </div>
  
  
  <div class="col-4 sm-width-full left mt-3">
    <a class="no-underline-hover py-1 block" href="http://localhost:4000/2017/01/21/python-linear-models-for-continuous-target.html">
      <span class="h5 bold">Next</span>
      <p class="bold h3 link-primary mb-1">Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target</p>
      <p>```python import sys print("Python version: {}".format(sys.version)) import pandas as pd print("pandas version: {}".format(pd.__version__)) import matplotlib print("matplotlib version: {}".format(matplotlib.__version__)) import numpy...</p>
    </a>
  </div>


</div>

    </div>

    <div class="container mx-auto clearfix mt-2 mt-lg-4 px-2">
  <div class="border-top-thick">
    <p class="col-8 sm-width-full left py-2 mb-0">This project is maintained by <a class="text-accent" href="https://github.com/ElvinOuyang">ElvinOuyang</a></p>
    <ul class="list-reset right clearfix sm-width-full py-2 mb-2 mb-lg-0">
      <li class="inline-block mr-1">
        <a href="https://twitter.com/share" class="twitter-share-button" data-hashtags="Elvin Ouyang's Blog">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li class="inline-block">
        <a class="github-button" href="https://github.com/ElvinOuyang/elvinouyang.github.io" data-icon="octicon-star" data-count-href="ElvinOuyang//stargazers" data-count-api="/repos/ElvinOuyang/#stargazers_count" data-count-aria-label="# stargazers on GitHub" aria-label="Star ElvinOuyang/ on GitHub">Star</a>
      </li>
    </ul>
  </div>
</div>


  </body>

</html>
