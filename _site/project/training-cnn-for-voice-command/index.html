<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.3.1 by Michael Rose
  Copyright 2017 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->









<title>            Kaggle TensorFlow Speech Recognition Challenge: Training Deep Neural Network for Voice Recognition      Elvin Ouyang’s Blog      </title>




<meta name="description" content="In this report, I will introduce my work for our Deep Learning final project. Our project is to finish the Kaggle Tensorflow Speech Recognition Challenge, where we need to predict the pronounced word from the recorded 1-second audio clips. To learn more about my work on this project, please visit my GitHub project page here.">




<meta name="author" content="Chuanye (Elvin) Ouyang">

<meta property="og:locale" content="en">
<meta property="og:site_name" content="Elvin Ouyang's Blog">
<meta property="og:title" content="Kaggle TensorFlow Speech Recognition Challenge: Training Deep Neural Network for Voice Recognition">


  <link rel="canonical" href="http://localhost:4000/project/training-cnn-for-voice-command/">
  <meta property="og:url" content="http://localhost:4000/project/training-cnn-for-voice-command/">



  <meta property="og:description" content="In this report, I will introduce my work for our Deep Learning final project. Our project is to finish the Kaggle Tensorflow Speech Recognition Challenge, where we need to predict the pronounced word from the recorded 1-second audio clips. To learn more about my work on this project, please visit my GitHub project page here.">





  

  



  <meta property="og:image" content="http://localhost:4000/assets/images/voice-rec-cnn/spectrogram_samples.png">



  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2017-12-10T00:00:00-05:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Elvin Ouyang",
      "url" : "http://localhost:4000",
      "sameAs" : ["https://twitter.com/elvinouyang","https://www.linkedin.com/in/ouyangchuanye/"]
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Elvin Ouyang's Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->

<meta http-equiv="cleartype" content="on">
    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->
  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="http://localhost:4000/">Elvin Ouyang's Blog</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/about/">About</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/projects/">Projects</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/study-notes/">Study Notes</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/categories/">Category</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/tags/">Tag</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/resources/">Resources</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/sitemap/">Sitemap</a></li>
          
        </ul>
        <button><div class="navicon"></div></button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    
  







<div class="page__hero"
  style=" "
>
  
    <img src="http://localhost:4000/assets/images/voice-rec-cnn/spectrogram_samples.png" alt="Kaggle TensorFlow Speech Recognition Challenge: Training Deep Neural Network for Voice Recognition" class="page__hero-image">
  
  
    <span class="page__hero-caption">Spectrograms from WAV files
</span>
  
</div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="http://localhost:4000/assets/images/bio-photo.jpeg" class="author__avatar" alt="Chuanye (Elvin) Ouyang" itemprop="image">
      
    </div>
  

  <div class="author__content">
    <h3 class="author__name" itemprop="name">Chuanye (Elvin) Ouyang</h3>
    
      <p class="author__bio" itemprop="description">
        Chuanye (Elvin) Ouyang is a young professional in data science, risk analysis, compliance evaluation, and forensic analytics.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> <span itemprop="name">Washington, DC</span>
        </li>
      

      

      

      

      
        <li>
          <a href="https://twitter.com/elvinouyang" itemprop="sameAs">
            <i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter
          </a>
        </li>
      

      

      

      
        <li>
          <a href="https://www.linkedin.com/in/ouyangchuanye" itemprop="sameAs">
            <i class="fa fa-fw fa-linkedin-square" aria-hidden="true"></i> LinkedIn
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://github.com/ElvinOuyang" itemprop="sameAs">
            <i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fa fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Kaggle TensorFlow Speech Recognition Challenge: Training Deep Neural Network for Voice Recognition">
    <meta itemprop="description" content="In this report, I will introduce my work for our Deep Learning final project. Our project is to finish the Kaggle Tensorflow Speech Recognition Challenge, where we need to predict the pronounced word from the recorded 1-second audio clips. To learn more about my work on this project, please visit my GitHub project page here.">
    <meta itemprop="datePublished" content="December 10, 2017">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Kaggle TensorFlow Speech Recognition Challenge: Training Deep Neural Network for Voice Recognition
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 




  12 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        <p>In this report, I will introduce my work for our Deep Learning final project. Our project is to finish the <a href="https://www.kaggle.com/c/tensorflow-speech-recognition-challenge">Kaggle Tensorflow Speech Recognition Challenge</a>, where we need to predict the pronounced word from the recorded 1-second audio clips. To learn more about my work on this project, please visit my GitHub project page <a href="https://github.com/ElvinOuyang/kaggle-tensorflow-speech-recognition-challenge">here</a>.</p>

<p>In our first research stage, we will <strong>turn each WAV file into MFCC vector of the same dimension</strong> (the files are of the same length). In the first few hidden layers (of either multi-layer perceptron or 1-D convolutional neural net), we plan to turn the MFCC vectors into log probability of phonemes, i.e. the basic building blocks of a pronounced word. We then plan to feed these sequences to a recurrent neural network (either a RNN or a more advanced LSTM) to train and predict the word. The assumption of this approach is that the MFCC values of a sound clip should reflect the nuance sequence in word pronunciation and the the sequence is strictly ordered. Therefore, the sequence should be be used in recurrent neural networks to classify the words.</p>

<p>In our second research stage, we will <strong>turn each WAV file into a visual graph (called spectrogram) of the same size</strong>. Since the graphical representation of the voice has pixel points of the same scale on two dimensions, we will then apply convolutional layers on the graphs to extract latent graphical patterns from the files. We will then build fully connected layers to link the extracted feature maps to the expected output. It is even possible to feed the extracted features as a sequence to a recurrent layer since the graphical patterns should also be strictly related to time series, but the model might be too complicated for the simple task we are dealing with. The assumption of this approach is that the graphical patterns in the spectrograms of different words pronounced in the WAV files should be typical enough for the convolutional neural network to train on.</p>

<h2 id="stage-1-proof-of-concept-training-for-mfcc-phonemes-approach">STAGE 1: Proof-of-concept training for MFCC-phonemes approach</h2>

<h3 id="stage-11-pre-training-an-mlp-for-mfcc-phonemes-layers">STAGE 1.1: Pre-training an MLP for MFCC-phonemes layers</h3>

<p>In our first stage, I will pre-train a model that can give a somewhat accurate probability estimation on phonemes based on MFCC values. The pre-training stage will help improve the complete model’s performance since the weights of corresponding layers are not randomly initiated. My first stab of this problem is a simple-structured MLP:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">MLP</span> <span class="p">(</span>
  <span class="p">(</span><span class="n">l1</span><span class="p">):</span> <span class="n">Linear</span> <span class="p">(</span><span class="n">num_inputs</span> <span class="o">-&gt;</span> <span class="mi">50</span><span class="p">)</span>
  <span class="p">(</span><span class="n">t1</span><span class="p">):</span> <span class="n">Tanh</span> <span class="p">()</span>
  <span class="p">(</span><span class="n">l2</span><span class="p">):</span> <span class="n">Linear</span> <span class="p">(</span><span class="mi">50</span> <span class="o">-&gt;</span> <span class="mi">38</span><span class="p">)</span>
  <span class="p">(</span><span class="n">t2</span><span class="p">):</span> <span class="n">LogSoftmax</span> <span class="p">()</span>
<span class="p">)</span>
</code></pre>
</div>

<p>The <code class="highlighter-rouge">num_inputs</code> value is determined by our feature generation stage when the MFCC scores of the same length are calculated for each WAV file. The target phonemes have 38 possible labels. With a learning rate of 0.1 and a momentum of 0.9 for the Stochastic Gradient Descent optimization algorithm, I created a model with 51.75% accuracy. This performance is quite impressive for such a simple model, considering the natural probability of the classification being correct is only 1/38, i.e. 2.63%. The training logs of the first 100 epochs is displayed below:</p>

<p><img src="/assets/images/voice-rec-cnn/training_log2017_12_04_04_29_epochs_100.png" alt="mlp_training_log" /></p>

<h3 id="stage-12-pre-training-a-cnn-for-mfcc-phonemes-layers">STAGE 1.2: Pre-training a CNN for MFCC-phonemes layers</h3>

<p>As an alternative to the MLP training, I also built a multi-layer convolutional neural network with 1-D kernels for each MFCC vector inputs to see if I can pre-train a better model than the MLP. Since the 39 features of MFCC vectors contain MFCC scores, deltas, and delta-deltas, I restructured the input tensor to <strong>3 by 13, where 3 is the number of channels</strong>. By restructuring input tensors in this way, I can ensure that the three types of data are updated separately in the back propagation process. The CNN network has a structure as below:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_channels</span> <span class="o">=</span> <span class="n">num_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_inputs</span> <span class="o">/</span> <span class="n">num_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">num_channels</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="c"># 96 is calculated based on 13 features per channel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">38</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_length</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># feature length 10</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># feature length 8</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># feature length 4</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># feature length 2</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># feature length 1, fc1 input is 1*96=96</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">*</span> <span class="mi">96</span><span class="p">)</span>  <span class="c"># flatten feature maps to 1D</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre>
</div>

<p>I also included pooling layers and dropout layers to ensure that the model do not overfit. The training logs of the first 50 epochs is displayed below:</p>

<p><img src="/assets/images/voice-rec-cnn/training_log_cnn_2017_12_04_18_48_epochs_50.png" alt="cnn_training_log" /></p>

<p>The model reached the highest accuracy at 55.55% at epoch 35.</p>

<h3 id="stage-13-concluding-the-mfcc-phonemes-approach">STAGE 1.3: Concluding the MFCC-phonemes approach</h3>

<p>The suboptimal result from the primary testing with MFCC and phonemes indicates that this traditional method is not the ideal approach that we should follow using neural network. I then seek to generate an easier network with our second approach.</p>

<h2 id="stage-2-training-a-cnn-for-spectrogram-word-predictions">STAGE 2: Training a CNN for spectrogram-word predictions</h2>

<h3 id="stage-21-generating-spectrograms-from-wav-files">STAGE 2.1: Generating spectrograms from .WAV files</h3>

<p>In order to train a convolutional network on graphical representation of .WAV files, I will need to preprocess the files. I used <code class="highlighter-rouge">scipy</code> package to generate the spectrograms in PNG format and generated a map file that includes the path of the PNG files and their corresponding labels. The core functions I used in this stage are as below:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_specgram</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">sample_rate</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
    <span class="n">nperseg</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">window_size</span> <span class="o">*</span> <span class="n">sample_rate</span> <span class="o">/</span> <span class="mf">1e3</span><span class="p">))</span>
    <span class="n">noverlap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">sample_rate</span> <span class="o">/</span> <span class="mf">1e3</span><span class="p">))</span>
    <span class="n">freqs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">spec</span> <span class="o">=</span> <span class="n">signal</span><span class="o">.</span><span class="n">spectrogram</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span>
                                        <span class="n">fs</span><span class="o">=</span><span class="n">sample_rate</span><span class="p">,</span>
                                        <span class="n">window</span><span class="o">=</span><span class="s">'hann'</span><span class="p">,</span>
                                        <span class="n">nperseg</span><span class="o">=</span><span class="n">nperseg</span><span class="p">,</span>
                                        <span class="n">noverlap</span><span class="o">=</span><span class="n">noverlap</span><span class="p">,</span>
                                        <span class="n">detrend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">freqs</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">wav2img</span><span class="p">(</span><span class="n">wav_path</span><span class="p">,</span> <span class="n">targetdir</span><span class="o">=</span><span class="s">''</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)):</span>
    <span class="s">"""
    takes in wave file path
    and the fig size. Default 4,4 will make images 288 x 288
    """</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">samplerate</span><span class="p">,</span> <span class="n">test_sound</span> <span class="o">=</span> <span class="n">wavfile</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">wav_path</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">spectrogram</span> <span class="o">=</span> <span class="n">log_specgram</span><span class="p">(</span><span class="n">test_sound</span><span class="p">,</span> <span class="n">samplerate</span><span class="p">)</span>
    <span class="n">output_file</span> <span class="o">=</span> <span class="n">wav_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'.wav'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">output_file</span> <span class="o">=</span> <span class="n">targetdir</span> <span class="o">+</span> <span class="s">'/'</span> <span class="o">+</span> <span class="n">output_file</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imsave</span><span class="p">(</span><span class="s">'</span><span class="si">%</span><span class="s">s.png'</span> <span class="o">%</span> <span class="n">output_file</span><span class="p">,</span> <span class="n">spectrogram</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">output_file</span> <span class="o">+</span> <span class="s">'.png'</span><span class="p">)</span>
</code></pre>
</div>

<p>The generated spectrograms are as below:</p>

<p><img src="/assets/images/voice-rec-cnn/spectrogram_samples.png" alt="spectrogram_samples" /></p>

<p>From these samples, I have seen that different words’ spectrograms do look differently. I am hopeful that we can train a convolutional network on these inputs! The other key part of my next training stage is the map file. The generated map file has contents as below:</p>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code>                                              path target
0  ../data/train/audio/right/988e2f9a_nohash_0.wav  right
1  ../data/train/audio/right/1eddce1d_nohash_3.wav  right
2  ../data/train/audio/right/93ec8b84_nohash_0.wav  right
3  ../data/train/audio/right/6272b231_nohash_1.wav  right
4  ../data/train/audio/right/439c84f4_nohash_1.wav  right
</code></pre>
</div>

<p>I will use the path and the target from this map file in my PyTorch stochastic mini batch training paradigm in the following stages.</p>

<h3 id="stage-22-building-initial-cnn-for-spectrogram-pictures-and-labels">STAGE 2.2: Building initial CNN for spectrogram pictures and labels</h3>

<p>In my training script, I customized <code class="highlighter-rouge">pytorch</code> package’s handy <code class="highlighter-rouge">DataSet</code> class and <code class="highlighter-rouge">DataLoader</code> class to achieve multi-CPU fast loading and easy GPU integration. The main script for the CNN training consists of the following steps:</p>

<ul>
  <li>Define hyper parameters for epochs, GPU usage, CPU usage, and optimization</li>
  <li>Load map file and define label index to replace labels with numbers</li>
  <li>Conduct train test split on map file</li>
  <li>Define spectrogram dataset and spectrogram PNG file transform methods so that PNG files and targets are automatically loaded and preprocessed into tensors of the same shape (batch_size * 3 * 99 * 161 and batch_size * 1)</li>
  <li>Create training and validating dataset and dataloader, where mini-batches are automatically generated using multiple CPUs</li>
  <li>Define CNN network structure, loss/criterion function, and optimization algorithm</li>
  <li>Define training, validating, and testing functions</li>
  <li>Write epoch loops with training, validation, and log recording</li>
</ul>

<p>The core part of the script, i.e. the CNN network framework, is defined as below:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="mi">18</span> <span class="o">*</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># feature shape 96 * 158</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># feature shape 48 * 79</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># feature shape 46 * 77</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>  <span class="c"># feature shape 23 * 39</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># feature shape 20 * 36</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># feature shape 10 * 18</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">18</span> <span class="o">*</span> <span class="mi">96</span><span class="p">)</span>  <span class="c"># flatten feature maps to 1D</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre>
</div>

<p>Be noted that the model takes three layers of kernels to reduce the input size to reasonably-sized 10 by 18 feature maps before being fed to fully connected layers for prediction. The script takes <code class="highlighter-rouge">num_workers</code> and <code class="highlighter-rouge">use_gpu</code> parameters that define number of CPUs for batch loading and whether to use GPUs for model training, separately. I ran the model on an instance on the Google Cloud Platform with 1 GPU and 10 CPUs; training the model locally with no GPUs is likely to take too much time to finish.</p>

<h3 id="stage-23-fine-tuning-model-with-class-weight-and-optimization-hyper-parameters">STAGE 2.3: Fine tuning model with class weight and optimization hyper parameters</h3>

<p>It is worth noting that the project only needs 11 tags for the output prediction, while the training input has more than 30 word tags. I had to relabel most of the word tags into “unknown” to match the desired output. However, after I relabeled the target by replacing non-voice-command words with a single label “unknown”, I observed an unbalanced dataset where samples with the “unknown” label is 17.44 times the size of other samples. When trained with this unbalanced data, I noticed that the trained CNN model will simply label all samples with the ‘unknown’ label to achieve the “best” NLLLoss score, such as below:</p>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code><span class="c"># first row is the prediction for 'unknown'</span>
Confusion Matrix:
<span class="o">[[</span>4080    0    0    0    0    0    0    0    0    0    0]
 <span class="o">[</span> 248    0    0    0    0    0    0    0    0    0    0]
 <span class="o">[</span> 264    0    0    0    0    0    0    0    0    0    0]
 <span class="o">[</span> 238    0    0    0    0    0    0    0    0    0    0]
 <span class="o">[</span> 219    0    0    0    0    0    0    0    0    0    0]
 <span class="o">[</span> 243    0    0    0    0    0    0    0    0    0    0]
 <span class="o">[</span> 230    0    0    0    0    0    0    0    0    0    0]
 <span class="o">[</span> 239    0    0    0    0    0    0    0    0    0    0]
 <span class="o">[</span> 225    0    0    0    0    0    0    0    0    0    0]
 <span class="o">[</span> 256    0    0    0    0    0    0    0    0    0    0]
 <span class="o">[</span> 231    0    0    0    0    0    0    0    0    0    0]]
</code></pre>
</div>

<p>To deal with the unbalanced data in the training set, I therefore implemented a class weight to penalize the predictions for ‘unknown’ class so that all labels have an equal weight regardless of the sample size. Hence my weight tensor is defined as <code class="highlighter-rouge">label_weight = torch.FloatTensor([0.057, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])</code>, where 0.057 equals 1/17.44. After adjusting for the class weights, I received the following confusion matrix early in the training epochs with a NLLoss of 0.41:</p>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code>Confusion Matrix:
<span class="o">[[</span>1516 1358  203  111   95   58  497  104   22   22   94]
 <span class="o">[</span>   1  230    6    2    5    1    1    0    2    0    0]
 <span class="o">[</span>   1   96   95    1   63    2    6    0    0    0    0]
 <span class="o">[</span>   5   18    3  175   13    2    5    2    0    2   13]
 <span class="o">[</span>   2   78   18    1  116    1    2    0    0    0    1]
 <span class="o">[</span>   2    9    9    2    0  140   69    0    0   12    0]
 <span class="o">[</span>   2   15    2    1    0    3  204    2    1    0    0]
 <span class="o">[</span>  28    9    8   13    0    0    5  175    0    0    1]
 <span class="o">[</span>   4   50   11    2    0    7    6    0  137    8    0]
 <span class="o">[</span>   4   19   22    3    2   33   30    1    4  138    0]
 <span class="o">[</span>   5   13    1    3    2    0    2    2    0    0  203]]
</code></pre>
</div>

<p>This looks much like what a confusion matrix is ‘supposed’ to be like: the model is actually learning from the input data now that the target is better defined. I then proceeded to hyper-parameter tuning, starting with a 5-epoch run with the standard <strong>learning rate at 0.1 and momentum at 0.9</strong>, as shown below:</p>

<p><img src="/assets/images/voice-rec-cnn/training_log_cnn_2017_12_11_15_31_epochs_5.png" alt="weighted_lr_0.1_m_0.9" /></p>

<p>We can see from this chart that the model overshot the global minimum after epoch 2 since the NLLLoss is steadily increasing afterwards. I will need to adjust the hyper-parameter to make the model learn much more slowly. Therefore, in my second 5-epoch run I defined <strong>learning rate as 0.005 and momentum as 0.95</strong>; my result is displayed below:</p>

<p><img src="/assets/images/voice-rec-cnn/training_log_cnn_2017_12_11_15_51_epochs_5.png" alt="weighted_lr_0.005_m_0.95" /></p>

<p>With a smaller gradient descent step and a larger jump over local minimums, we can clearly see that the model is learning steadily from the input data this time: the NLLLoss is steadily decreasing and the accuracy is steadily increasing. Therefore, I decided that these combination of hyper-parameter is what we desired and went forward to train the data with 30 epochs. My result is displayed below:</p>

<p><img src="/assets/images/voice-rec-cnn/training_log_cnn_2017_12_11_16_30_epochs_30.png" alt="weighted_lr_0.005_m_0.95" /></p>

<p>Within 30 epochs, we have our highest accuracy at epoch 25 with the confusion matrix as below:</p>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code><span class="o">[</span>Epoch 25] Accuracy: 91.24%, Average Loss: 0.15
Confusion Matrix:
<span class="o">[[</span>3753   30   77   11   22   38   53   56   20   12    8]
 <span class="o">[</span>  13  221    9    1    1    0    1    0    2    0    0]
 <span class="o">[</span>   5    7  231    2   14    0    2    1    1    1    0]
 <span class="o">[</span>  19    0    0  216    0    0    0    1    0    0    2]
 <span class="o">[</span>   7    6   20    1  181    0    0    1    1    2    0]
 <span class="o">[</span>   8    0    1    1    1  215    6    0    1   10    0]
 <span class="o">[</span>  14    0    1    0    0    2  212    0    0    1    0]
 <span class="o">[</span>   8    0    1    1    1    0    0  228    0    0    0]
 <span class="o">[</span>   6    2    2    1    0    4    2    1  200    7    0]
 <span class="o">[</span>   6    1    2    1    0   14    1    0    4  227    0]
 <span class="o">[</span>   9    0    0    0    0    0    0    0    0    0  222]]
</code></pre>
</div>

<p>The best average NLLLoss, however, was achieved at epoch 17 with the confusion matrix as below:</p>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code><span class="o">[</span>Epoch 17] Accuracy: 86.50%, Average Loss: 0.12
Confusion Matrix:
<span class="o">[[</span>3431   47   54   73   60   88   80  110   49   37   51]
 <span class="o">[</span>   9  218    3    0   10    2    2    0    3    0    1]
 <span class="o">[</span>   2    7  218    3   25    2    3    1    1    2    0]
 <span class="o">[</span>   5    0    0  224    2    1    0    1    0    0    5]
 <span class="o">[</span>   5    7   10    3  187    0    0    0    1    4    2]
 <span class="o">[</span>   4    1    0    2    0  222    5    0    1    8    0]
 <span class="o">[</span>   6    1    0    0    0    6  214    0    0    2    1]
 <span class="o">[</span>   3    0    1    3    1    0    0  231    0    0    0]
 <span class="o">[</span>   2    3    1    1    0    6    2    0  205    5    0]
 <span class="o">[</span>   4    1    1    2    0   16    1    0    6  225    0]
 <span class="o">[</span>   2    0    0    2    0    0    1    1    1    0  224]]
</code></pre>
</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/tags/#artificial-intelligence" class="page__taxonomy-item" rel="tag">artificial-intelligence</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#deep-learning" class="page__taxonomy-item" rel="tag">deep-learning</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#neural-network" class="page__taxonomy-item" rel="tag">neural-network</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#pytorch" class="page__taxonomy-item" rel="tag">pytorch</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#voice-recognition" class="page__taxonomy-item" rel="tag">voice-recognition</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/categories/#project" class="page__taxonomy-item" rel="tag">Project</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-12-10T00:00:00-05:00">December 10, 2017</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="http://localhost:4000/project/creating-a-multilayer-perceptron-with-lua-torch7/" class="pagination--pager" title="Creating a multi-layer perceptron to train on MNIST dataset
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
    <h4 class="page__comments-title">Leave a Comment</h4>
    <section id="disqus_thread"></section>
  
</div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/ElvinOuyang"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2017 Elvin Ouyang. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-96112090-1', 'auto');
  ga('send', 'pageview');
</script>







  
  <script type="text/javascript">
  	/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  	var disqus_shortname = 'elvinouyang-github-io';

  	/* * * DON'T EDIT BELOW THIS LINE * * */
  	(function() {
  		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  	})();

  	/* * * DON'T EDIT BELOW THIS LINE * * */
  	(function () {
  		var s = document.createElement('script'); s.async = true;
  		s.type = 'text/javascript';
  		s.src = '//' + disqus_shortname + '.disqus.com/count.js';
  		(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
  	}());
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>






  </body>
</html>
