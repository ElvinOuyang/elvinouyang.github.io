<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.3.1 by Michael Rose
  Copyright 2017 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->









<title>            Applying Bag of Words and Word2Vec models on Reuters-21578 Dataset      Elvin Ouyang’s Blog      </title>




<meta name="description" content="Introduction">




<meta name="author" content="Chuanye (Elvin) Ouyang">

<meta property="og:locale" content="en">
<meta property="og:site_name" content="Elvin Ouyang's Blog">
<meta property="og:title" content="Applying Bag of Words and Word2Vec models on Reuters-21578 Dataset">


  <link rel="canonical" href="http://localhost:4000/project/reuters-w2v-bow-get-started/">
  <meta property="og:url" content="http://localhost:4000/project/reuters-w2v-bow-get-started/">



  <meta property="og:description" content="Introduction">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2017-09-15T00:00:00-04:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Elvin Ouyang",
      "url" : "http://localhost:4000",
      "sameAs" : ["https://twitter.com/elvinouyang","https://www.linkedin.com/in/ouyangchuanye/"]
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Elvin Ouyang's Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->

<meta http-equiv="cleartype" content="on">
    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->
  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="http://localhost:4000/">Elvin Ouyang's Blog</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/about/">About</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/projects/">Projects</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/study-notes/">Study Notes</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/categories/">Category</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/tags/">Tag</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/resources/">Resources</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/sitemap/">Sitemap</a></li>
          
        </ul>
        <button><div class="navicon"></div></button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    



<div id="main" role="main">
  
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="http://localhost:4000/assets/images/bio-photo.jpeg" class="author__avatar" alt="Chuanye (Elvin) Ouyang" itemprop="image">
      
    </div>
  

  <div class="author__content">
    <h3 class="author__name" itemprop="name">Chuanye (Elvin) Ouyang</h3>
    
      <p class="author__bio" itemprop="description">
        Chuanye (Elvin) Ouyang is a young professional in data science, risk analysis, compliance evaluation, and forensic analytics.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> <span itemprop="name">Washington, DC</span>
        </li>
      

      

      

      

      
        <li>
          <a href="https://twitter.com/elvinouyang" itemprop="sameAs">
            <i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter
          </a>
        </li>
      

      

      

      
        <li>
          <a href="https://www.linkedin.com/in/ouyangchuanye" itemprop="sameAs">
            <i class="fa fa-fw fa-linkedin-square" aria-hidden="true"></i> LinkedIn
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://github.com/ElvinOuyang" itemprop="sameAs">
            <i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fa fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Applying Bag of Words and Word2Vec models on Reuters-21578 Dataset">
    <meta itemprop="description" content="Introduction">
    <meta itemprop="datePublished" content="September 15, 2017">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Applying Bag of Words and Word2Vec models on Reuters-21578 Dataset
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 




  11 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        <h2 id="introduction">Introduction</h2>

<p>In this post, I will showcase the steps I took to create a continuous vector space based on the corpora included in the famous <a href="https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection">Reuters-21578 dataset</a> (hereafter ‘reuters dataset’). The reuters dataset is a tagged text corpora with news excerpts from Reuters newswire in 1987. Although the contents of the news is somewhat outdated, the topic labels provided in this dataset is widely used as a benchmark for supervised learning tasks that involve natural language processing (NLP).</p>

<p><a href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a>, among other word embedding models, is a neural network model that aims to generate numerical feature vectors for words in a way that maintains the relative meanings of the words in the mapped vectors. It provides a reliable method to reduce the feature dimensionality of the data, which is the biggest challenge for traditional NLP models such as the Bag of Words (BOW) model.</p>

<p><a href="https://en.wikipedia.org/wiki/Bag-of-words_model">BOW</a>, on the other hand, is the traditional and established approach in text mining. The model deconstructs text into a list of words with either frequency or dummy checklist, hence creating a “bag of words” as a result. Although this approach is widely used as the beginner model for text-related projects, it has significant drawbacks:</p>

<ul>
  <li>The sequencing and relative positioning of the words are largely ignored</li>
  <li>The resulted features are sparse, making it harder to build predictive models with</li>
</ul>

<p>Although these two drawbacks could be fixed with n-gram and dimensionality reduction, it is an analyst’s dream to have a model like Word2Vec that gives you sensible vector representation of words that’s useful for semantic analysis.</p>

<h2 id="load-reuters-dataset">Load Reuters Dataset</h2>

<p>Before applying the models, I will generate a dataframe with the corpora. The first step is to download the reuters data and check out what is inside.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">reuters</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="c"># quick summary of the reuters corpus</span>
<span class="k">print</span><span class="p">(</span><span class="s">"&gt;&gt;&gt; The reuters corpus has {} tags"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">reuters</span><span class="o">.</span><span class="n">categories</span><span class="p">())))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"&gt;&gt;&gt; The reuters corpus has {} documents"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">reuters</span><span class="o">.</span><span class="n">fileids</span><span class="p">())))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>&gt;&gt;&gt; The reuters corpus has 90 tags
&gt;&gt;&gt; The reuters corpus has 10788 documents
</code></pre>
</div>

<p>The <code class="highlighter-rouge">reuters</code> from <code class="highlighter-rouge">nltk</code> package comes with its specific sets of methods, making it hard to select and filter desired corpora. For my purpose in this report, I will only select 2 popular tags out of the 90 tags included in this corpora. Therefore, my next step is to generate a frequency table that summarizes the tags.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># create counter to summarize</span>
<span class="n">categories</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">file_count</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c"># count each tag's number of documents</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reuters</span><span class="o">.</span><span class="n">categories</span><span class="p">():</span>
    <span class="s">"""print("$ There are {} documents included in topic </span><span class="se">\"</span><span class="s">{}</span><span class="se">\"</span><span class="s">"
          .format(len(reuters.fileids(i)), i))"""</span>
    <span class="n">file_count</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">reuters</span><span class="o">.</span><span class="n">fileids</span><span class="p">(</span><span class="n">i</span><span class="p">)))</span>
    <span class="n">categories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

<span class="c"># create a dataframe out of the counts</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s">'categories'</span><span class="p">:</span> <span class="n">categories</span><span class="p">,</span> <span class="s">"file_count"</span><span class="p">:</span> <span class="n">file_count</span><span class="p">})</span> \
    <span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">'file_count'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>   categories  file_count
21       earn        3964
0         acq        2369
46   money-fx         717
26      grain         582
17      crude         578
</code></pre>
</div>

<p>I decide to chose the <strong>second and third</strong> tags on this top tags list, since the first <strong>earn</strong> tag is most likely the highly-standardized news pieces with earnings reports.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Select documents that only contains top two labels with most documents</span>
<span class="n">cat_start</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">category_filter</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">cat_start</span><span class="p">:</span><span class="n">cat_end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">"&gt;&gt;&gt; The following categories are selected for the analysis: </span><span class="se">\
</span><span class="s">      {category_filter}"</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>&gt;&gt;&gt; The following categories are selected for the analysis:       ['acq', 'money-fx']
</code></pre>
</div>

<p>I then apply my tag filter on the text corpora to select text that have either the <code class="highlighter-rouge">acq</code> tag or the <code class="highlighter-rouge">money-fx</code> tag. The reuters data comes with a split of training and testing itself, so I will stick with its original split.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># select fileid with the category filter</span>
<span class="n">doc_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">reuters</span><span class="o">.</span><span class="n">fileids</span><span class="p">(</span><span class="n">category_filter</span><span class="p">))</span>
<span class="n">doc_list</span> <span class="o">=</span> <span class="n">doc_list</span><span class="p">[</span><span class="n">doc_list</span> <span class="o">!=</span> <span class="s">'training/3267'</span><span class="p">]</span>

<span class="n">test_doc</span> <span class="o">=</span> <span class="n">doc_list</span><span class="p">[[</span><span class="s">'test'</span> <span class="ow">in</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">doc_list</span><span class="p">]]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"&gt;&gt;&gt; test_doc is created with following document names: {} ..."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_doc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]))</span>
<span class="n">train_doc</span> <span class="o">=</span> <span class="n">doc_list</span><span class="p">[[</span><span class="s">'training'</span> <span class="ow">in</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">doc_list</span><span class="p">]]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"&gt;&gt;&gt; train_doc is created with following document names: {} ..."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_doc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]))</span>

<span class="n">test_corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s">" "</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">reuters</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="n">test_doc</span><span class="p">[</span><span class="n">t</span><span class="p">])])</span>
               <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_doc</span><span class="p">))]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"&gt;&gt;&gt; test_corpus is created, the first line is: {} ..."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">100</span><span class="p">]))</span>
<span class="n">train_corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s">" "</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">reuters</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="n">train_doc</span><span class="p">[</span><span class="n">t</span><span class="p">])])</span>
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_doc</span><span class="p">))]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"&gt;&gt;&gt; train_corpus is created, the first line is: {} ..."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">100</span><span class="p">]))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>&gt;&gt;&gt; test_doc is created with following document names: ['test/14843' 'test/14849' 'test/14852' 'test/14861' 'test/14865'] ...
&gt;&gt;&gt; train_doc is created with following document names: ['training/10' 'training/1000' 'training/10005' 'training/10018'
 'training/10025'] ...
&gt;&gt;&gt; test_corpus is created, the first line is: SUMITOMO BANK AIMS AT QUICK RECOVERY FROM MERGER Sumitomo Bank Ltd &amp; lt ; SUMI . T &gt; is certain to l ...
&gt;&gt;&gt; train_corpus is created, the first line is: COMPUTER TERMINAL SYSTEMS &amp; lt ; CPML &gt; COMPLETES SALE Computer Terminal Systems Inc said it has com ...
</code></pre>
</div>

<p>Now that I have the corpora in place, it’s time to clean up the text to reduce the noice from non-text elements. I have stored my text-cleaning functions in another module called <code class="highlighter-rouge">text_clean.py</code> and I will import it to clean up the text. My module document as well as all codes related to this project can be found at my GitHub Repo <a href="https://github.com/ElvinOuyang/reuters-w2v-practice">here</a>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">text_clean</span> <span class="kn">as</span> <span class="nn">tc</span>

<span class="c"># create clean corpus for word2vec approach</span>
<span class="n">test_clean_string</span> <span class="o">=</span> <span class="n">tc</span><span class="o">.</span><span class="n">clean_corpus</span><span class="p">(</span><span class="n">test_corpus</span><span class="p">)</span>
<span class="n">train_clean_string</span> <span class="o">=</span> <span class="n">tc</span><span class="o">.</span><span class="n">clean_corpus</span><span class="p">(</span><span class="n">train_corpus</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'&gt;&gt;&gt; The first few words from cleaned test_clean_string is: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_clean_string</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">100</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'&gt;&gt;&gt; The first few words from cleaned train_clean_string is: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_clean_string</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">100</span><span class="p">]))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>&gt;&gt;&gt;&gt; response cleaning initiated
&gt;&gt;&gt;&gt; cleaning response #500 out of 898
&gt;&gt;&gt;&gt; response cleaning initiated
&gt;&gt;&gt;&gt; cleaning response #500 out of 2186
&gt;&gt;&gt;&gt; cleaning response #1000 out of 2186
&gt;&gt;&gt;&gt; cleaning response #1500 out of 2186
&gt;&gt;&gt;&gt; cleaning response #2000 out of 2186
&gt;&gt;&gt; The first few words from cleaned test_clean_string is: sumitomo bank aim quick recovery merger sumitomo bank ltd lt sumi certain lose status japan profitab
&gt;&gt;&gt; The first few words from cleaned train_clean_string is: computer terminal systems lt cpml complete sale computer terminal systems inc say complete sale shar
</code></pre>
</div>

<h2 id="glimpse-of-bow-model">Glimpse of BOW model</h2>
<p>After the text is cleaned, I can now apply the BOW model on the corpora. The BOW is basically a frequency <code class="highlighter-rouge">Counter</code>, so I have written a function to get BOW from the corpora in my <code class="highlighter-rouge">text_clean.py</code> module.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># create clean corpus for bow approach</span>
<span class="n">test_clean_token</span> <span class="o">=</span> <span class="n">tc</span><span class="o">.</span><span class="n">clean_corpus</span><span class="p">(</span><span class="n">test_corpus</span><span class="p">,</span> <span class="n">string_line</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">train_clean_token</span> <span class="o">=</span> <span class="n">tc</span><span class="o">.</span><span class="n">clean_corpus</span><span class="p">(</span><span class="n">train_corpus</span><span class="p">,</span> <span class="n">string_line</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c"># quick look at the word frequency</span>
<span class="n">test_bow</span><span class="p">,</span> <span class="n">test_word_freq</span> <span class="o">=</span> <span class="n">tc</span><span class="o">.</span><span class="n">get_bow</span><span class="p">(</span><span class="n">test_clean_token</span><span class="p">)</span>
<span class="n">train_bow</span><span class="p">,</span> <span class="n">train_word_freq</span> <span class="o">=</span> <span class="n">tc</span><span class="o">.</span><span class="n">get_bow</span><span class="p">(</span><span class="n">train_clean_token</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>&gt;&gt;&gt;&gt; response cleaning initiated
&gt;&gt;&gt;&gt; cleaning response #500 out of 898
&gt;&gt;&gt;&gt; response cleaning initiated
&gt;&gt;&gt;&gt; cleaning response #500 out of 2186
&gt;&gt;&gt;&gt; cleaning response #1000 out of 2186
&gt;&gt;&gt;&gt; cleaning response #1500 out of 2186
&gt;&gt;&gt;&gt; cleaning response #2000 out of 2186
This corpus has 7126 key words, and the 10 most frequent words are: [('say', 2976), ('lt', 1112), ('share', 1067), ('dlrs', 921), ('company', 886), ('pct', 758), ('mln', 755), ('inc', 637), ('bank', 505), ('corp', 500)]
This corpus has 11042 key words, and the 10 most frequent words are: [('say', 7388), ('lt', 2802), ('share', 2306), ('dlrs', 2247), ('mln', 2172), ('pct', 2051), ('bank', 1983), ('company', 1942), ('inc', 1469), ('u', 1291)]
</code></pre>
</div>

<p>The BOW is a frequency table that records the word frequencies for each news piece in the corpora. If I turn it into a table view, it is clear that the most of the columns will be <code class="highlighter-rouge">NaN</code> due to the sparse nature of a BOW model. In fact, for the few rows of the <code class="highlighter-rouge">test_bow</code> table, all displayed cells below are NaN. Tables like this will make most of the data science models imprecise when we conduct predictive analysis.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_bow</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>   aabex  aame  aar  ab  abandon  abate  abatement  abboud  abegglen  abeles  \
0    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   
1    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   
2    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   
3    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   
4    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   

     ...     zellerbach  zenex  zinn  zoete  zond  zondervan  zone  zoran  \
0    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   
1    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   
2    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   
3    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   
4    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   

   zurich  zwermann  
0     NaN       NaN  
1     NaN       NaN  
2     NaN       NaN  
3     NaN       NaN  
4     NaN       NaN  

[5 rows x 7126 columns]
</code></pre>
</div>

<p>The typical way to reduce the noise in BOW models is to use dimensionality reduction methods, such as <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing">Latent Semantic Indexing (LSA)</a>, <a href="http://users.ics.aalto.fi/ella/publications/randproj_kdd.pdf">Random Projections (RP)</a>, <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet Allocation (LDA)</a>, or <a href="http://proceedings.mlr.press/v15/wang11a/wang11a.pdf">Hierachical Dirichlet Process (HDP)</a>.</p>

<h2 id="glimpse-of-word2vec-model">Glimpse of Word2Vec Model</h2>
<p>The alternative, of course, is to use the famous word2vec algorithm to generate continous numeric vectors. I will use <code class="highlighter-rouge">gensim</code> package to conduct this task. After I train the model with the reuters corpora, I get a dictionary that maps a numeric vector to each word that appears in the corpora.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="c"># set up logging for gensim training</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">format</span><span class="o">=</span><span class="s">'</span><span class="si">%(asctime)</span><span class="s">s : </span><span class="si">%(levelname)</span><span class="s">s : </span><span class="si">%(message)</span><span class="s">s'</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">train_clean_token</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>2017-10-17 10:41:29,134 : INFO : collecting all words and their counts
2017-10-17 10:41:29,135 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2017-10-17 10:41:29,197 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences
2017-10-17 10:41:29,198 : INFO : Loading a fresh vocabulary
2017-10-17 10:41:29,235 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)
2017-10-17 10:41:29,236 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)
2017-10-17 10:41:29,295 : INFO : deleting the raw counts dictionary of 11042 items
2017-10-17 10:41:29,297 : INFO : sample=0.001 downsamples 40 most-common words
2017-10-17 10:41:29,298 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)
2017-10-17 10:41:29,300 : INFO : estimated required memory for 11042 words and 100 dimensions: 14354600 bytes
2017-10-17 10:41:29,375 : INFO : resetting layer weights
2017-10-17 10:41:29,533 : INFO : training model with 2 workers on 11042 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2017-10-17 10:41:30,539 : INFO : PROGRESS: at 80.76% examples, 681629 words/s, in_qsize 3, out_qsize 0
2017-10-17 10:41:30,878 : INFO : worker thread finished; awaiting finish of 1 more threads
2017-10-17 10:41:30,887 : INFO : worker thread finished; awaiting finish of 0 more threads
2017-10-17 10:41:30,888 : INFO : training on 958850 raw words (846962 effective words) took 1.4s, 626379 effective words/s
</code></pre>
</div>

<p>With the model, I can get the numeric representation of any word that’s included in the model’s dictionary, and even quickly calculate the “similarity” between two words.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"&gt;&gt;&gt; Printing the vector of 'inc': {} ..."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s">'inc'</span><span class="p">][:</span><span class="mi">10</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"&gt;&gt;&gt; Printing the similarity between 'inc' and 'love': {}"</span>\
      <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'inc'</span><span class="p">,</span> <span class="s">'love'</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"&gt;&gt;&gt; Printing the similarity between 'inc' and 'company': {}"</span>\
      <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'inc'</span><span class="p">,</span> <span class="s">'company'</span><span class="p">)))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>&gt;&gt;&gt; Printing the vector of 'inc': [ 0.78641725 -0.81131667  2.96111465 -1.71037292  1.00312221  0.0688597
  0.13069828  0.99924785 -0.35323438  0.14066967] ...
&gt;&gt;&gt; Printing the similarity between 'inc' and 'love': 0.8529129076656723
&gt;&gt;&gt; Printing the similarity between 'inc' and 'company': 0.9254433388270853
</code></pre>
</div>

<p><em>inc</em> and <em>company</em> appears to be more similar than <em>inc</em> and <em>love</em>. After I get the vector representation of each word, I can calculate the vector representation of each news piece by simply getting the vector mean of all words included in that news piece. There certainly are more advanced aggregation methods, but for this practice I will use the most straightforward way. Another thing worth noticing is that I need to assign an all-zero vector if a word is not included in the dictionary of the Word2Vec model.</p>

<p>I also created a module for the matrix aggregation in my GitHub repo <a href="https://github.com/ElvinOuyang/reuters-w2v-practice">here</a>; so I simply import the function <code class="highlighter-rouge">get_doc_matrix</code> and calculate the document vectors.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">w2v_cal</span> <span class="kn">import</span> <span class="n">get_doc_matrix</span>

<span class="n">test_matrix</span> <span class="o">=</span> <span class="n">get_doc_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_clean_token</span><span class="p">)</span>
<span class="n">train_matrix</span> <span class="o">=</span> <span class="n">get_doc_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_clean_token</span><span class="p">)</span>
</code></pre>
</div>

<p>Be noted that my <code class="highlighter-rouge">word2vec</code> model was trained with the <code class="highlighter-rouge">train_clean_token</code> corpus, but I use the model on the <code class="highlighter-rouge">test_clean_token</code> here. I did it on purpose because in real-life situation, you won’t know your test set when you train the model. If you need to evaluate how accurate the model can be, you need to separate the train and test even on the word embedding stage.</p>

<p>A work-around that I’ve thought of is to update the <code class="highlighter-rouge">word2vec</code> model on the fly by feeding it the new corpora that come with the new documents: in this way I only update the model with the information that I know of - the test document I’m dealing with and all the train documents. I will further elaborate on this idea in my future posts about word embedding applications.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_matrix</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>         0         1         2         3         4         5         6   \
0  0.147100 -0.175438  0.888369 -0.069528  0.033833 -0.013307  0.233525   
1  0.042170 -0.163114  0.978680  0.227916 -0.142440 -0.083064  0.235953   
2  0.155263 -0.208363  1.048186 -0.157641  0.127218 -0.066839  0.140428   
3 -0.023057 -0.217250  1.123171  0.460213 -0.346473 -0.105230  0.137149   
4  0.183727 -0.297932  1.154441 -0.333167  0.194415 -0.019180  0.009379   

         7         8         9     ...           90        91        92  \
0  0.274196 -0.175343 -0.073920    ...     0.069914 -0.158157  0.241313   
1  0.349826 -0.203713 -0.163635    ...     0.014871 -0.315029  0.215814   
2  0.388008 -0.176918 -0.088959    ...     0.181733 -0.196467  0.211149   
3  0.438802 -0.264249 -0.317894    ...    -0.017868 -0.349207  0.047567   
4  0.449627 -0.152873 -0.030410    ...     0.272388 -0.197652  0.166376   

         93        94        95        96        97        98        99  
0 -0.301072 -0.613152 -0.001587  0.609080  0.174361 -0.124058 -0.203552  
1 -0.306695 -0.832693  0.124513  0.852931  0.039255 -0.264621 -0.344080  
2 -0.378385 -0.608279 -0.048504  0.716636  0.140785 -0.093196 -0.194501  
3 -0.359221 -0.989569  0.082096  1.062137 -0.133572 -0.353757 -0.346747  
4 -0.503684 -0.586876 -0.133392  0.807204  0.136802 -0.059075 -0.122065  

[5 rows x 100 columns]
</code></pre>
</div>

<p>A glimpse of the resulted word matrix indicates that the dataset is now filled with continuous numeric values. I can now use more models to predict the labels of these news pieces. I will test the effectiveness of these two models in my upcoming posts.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/tags/#gensim" class="page__taxonomy-item" rel="tag">gensim</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#natural-language-processing" class="page__taxonomy-item" rel="tag">Natural Language Processing</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#python" class="page__taxonomy-item" rel="tag">Python</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#word2vec" class="page__taxonomy-item" rel="tag">Word2Vec</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/categories/#project" class="page__taxonomy-item" rel="tag">Project</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-09-15T00:00:00-04:00">September 15, 2017</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="http://localhost:4000/tutorials/r-table-wide-to-long/" class="pagination--pager" title="Quick Guide to Create Frequency Table from Wide Table Using R
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
    <h4 class="page__comments-title">Leave a Comment</h4>
    <section id="disqus_thread"></section>
  
</div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/ElvinOuyang"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2017 Elvin Ouyang. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-96112090-1', 'auto');
  ga('send', 'pageview');
</script>







  
  <script type="text/javascript">
  	/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  	var disqus_shortname = 'elvinouyang-github-io';

  	/* * * DON'T EDIT BELOW THIS LINE * * */
  	(function() {
  		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  	})();

  	/* * * DON'T EDIT BELOW THIS LINE * * */
  	(function () {
  		var s = document.createElement('script'); s.async = true;
  		s.type = 'text/javascript';
  		s.src = '//' + disqus_shortname + '.disqus.com/count.js';
  		(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
  	}());
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>






  </body>
</html>
